# Fine-tuning T5 for Question Answering

## ğŸ‘¥ Team Information
**Course**: Deep Learning  
**Institution**: Telkom University  

| Name | NIM |
|------|-----|
| Alvito Kiflan Hawari | 1103220235 |
| Nafal Rifky Atsilah Maulana | 1103223106 |

---

## ğŸ¯ Purpose
This repository contains the implementation of a project focused on **Question Answering (QA)**.

The objective of this project is to fine-tune a pre-trained **T5 (Text-to-Text Transfer Transformer)** model to perform **extractive question answering**.  
Given a context paragraph and a question, the model learns to generate the correct answer span from the text.

---

## ğŸ” Project Overview

### The Task: Question Answering
We utilize the **SQuAD (Stanford Question Answering Dataset)**, a reading comprehension dataset consisting of questions posed by crowdworkers on a collection of Wikipedia articles.

### The Model: Google T5-base with Prefix-Tuning
- **Architecture**: Encoder-Decoder Transformer (Seq2Seq)
- **Approach**: Question Answering is treated as a **text-generation task**, where:
  - **Input**:  
    ```
    question: <question> context: <context>
    ```
  - **Output**:  
    ```
    <answer>
    ```
- **Fine-tuning Strategy**: This project employs **Prefix-Tuning**, which adapts the model by training only a small set of prefix tokens while keeping the base T5 model frozen. This approach is parameter-efficient and reduces training cost.

### The Dataset: SQuAD
- **Input**: Context paragraph and corresponding question
- **Output**: Exact answer span extracted from the context
- **Dataset Size Used**:
  - 10,000 training samples
  - 1,000 validation samples

---

## ğŸ“Š Technical Approach

### Model Configuration
- **Base Model**: `google/t5-base`
- **Tokenizer**: `T5TokenizerFast` (SentencePiece-based)
- **Framework**: PyTorch & Hugging Face Transformers
- **Fine-tuning Method**: Prefix-Tuning (only prefix tokens are trainable)


---
## Model Availability
Due to the large size of the model files (model.safetensors), the complete model checkpoint is available via Google Drive: https://drive.google.com/drive/folders/1R0p7kU86WpWRC8rg4aPRBa0D1VdtpJXd?usp=drive_link


---
Google Drive Link: https://drive.google.com/drive/folders/1JTbXIL85crFncUY5RvFHPZR0sEp-FQJX?usp=sharing
### Training Configuration
- **Batch Size**: 8
- **Learning Rate**: 5e-4
- **Epochs**: 5
- **Optimizer**: AdamW
- **Scheduler**: Linear learning rate schedule with warmup
- **Gradient Clipping**: Maximum norm of 1.0

---

## ğŸ“ Repository Structure

finetuning-t5-question-answering/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ task_2_deep_learning.ipynb # Main Jupyter Notebook for QA Training
â”‚
â”œâ”€â”€ finetune-t5-SQuAD/ # Directory for saved model checkpoints
â”‚ â””â”€â”€ best_model.pt # Best model checkpoint
â”‚
â””â”€â”€ README.md # Project documentation
